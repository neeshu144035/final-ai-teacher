{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78f20f5-4304-4408-aa11-83a0a7d742d8",
   "metadata": {},
   "source": [
    "# full textbook parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51634ba-6853-452e-b46a-fccd283729c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m pages_text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(pdf_file_path)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Step 2: Parse chapters and sections (with cleaning)\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m structured_content \u001b[38;5;241m=\u001b[39m \u001b[43mparse_chapters_and_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Step 3: Print preview in console\u001b[39;00m\n\u001b[0;32m    164\u001b[0m print_structured_content(structured_content)\n",
      "Cell \u001b[1;32mIn[2], line 96\u001b[0m, in \u001b[0;36mparse_chapters_and_sections\u001b[1;34m(pages_text)\u001b[0m\n\u001b[0;32m     94\u001b[0m     structured_content[current_chapter][current_heading] \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_heading \u001b[38;5;129;01mand\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m---> 96\u001b[0m     \u001b[43mstructured_content\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_chapter\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Clean and set heading\u001b[39;00m\n\u001b[0;32m     99\u001b[0m heading_number \u001b[38;5;241m=\u001b[39m heading_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# ------------------ CLEAN HEADING FUNCTION ------------------\n",
    "def clean_heading(text):\n",
    "    \"\"\"\n",
    "    Clean broken/duplicated headings and fix common OCR errors.\n",
    "    \"\"\"\n",
    "    corrections = {\n",
    "        \"CHEMIC\": \"CHEMICAL\",\n",
    "        \"CHEMIC1.1\": \"CHEMICAL\",  # Extra broken case\n",
    "        \"EQUA\": \"EQUATION\",\n",
    "        \"TIONSTIONS\": \"TIONS\",\n",
    "        \"EQUAAL\": \"EQUATION\",\n",
    "        \"AL\": \"\",  # Remove unnecessary 'AL' if broken\n",
    "    }\n",
    "\n",
    "    # Step 1: Remove repeated number pattern\n",
    "    text = re.sub(r'^(\\d+(?:\\.\\d+)+)\\s+(.*?)\\1.*', r'\\1 \\2', text)\n",
    "\n",
    "    # Step 2: Split words and remove exact duplicates\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    seen_words = set()\n",
    "\n",
    "    for word in words:\n",
    "        word = corrections.get(word, word)  # Apply corrections\n",
    "        if word not in seen_words:\n",
    "            cleaned_words.append(word)\n",
    "            seen_words.add(word)\n",
    "\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "\n",
    "# ------------------ PARSE FUNCTION ------------------\n",
    "def parse_chapters_and_sections(pages_text):\n",
    "    \"\"\"\n",
    "    Parses the extracted text into structured chapters and headings.\n",
    "    \"\"\"\n",
    "    structured_content = {}\n",
    "    current_chapter = None\n",
    "    current_heading = None\n",
    "    buffer = \"\"\n",
    "\n",
    "    # Regex patterns\n",
    "    chapter_pattern = re.compile(r'\\b(\\d+)\\s*CHAPTER\\b', re.IGNORECASE)\n",
    "    heading_pattern = re.compile(r'^(\\d+(?:\\.\\d+)+)\\s+([A-Z][^\\n]*)', re.IGNORECASE)\n",
    "\n",
    "    chapters_seen = set()  # Avoid TOC duplicates\n",
    "\n",
    "    for page in pages_text:\n",
    "        lines = page.split('\\n')\n",
    "        for line in lines:\n",
    "            line_clean = line.strip()\n",
    "            if not line_clean:\n",
    "                continue  # Skip empty lines\n",
    "\n",
    "            # --------- Skip TOC/Index pages ----------\n",
    "            if 'contents' in line_clean.lower() or 'index' in line_clean.lower():\n",
    "                continue\n",
    "\n",
    "            # ---------- Detect Chapter ----------\n",
    "            chapter_match = chapter_pattern.search(line_clean)\n",
    "            if chapter_match:\n",
    "                chapter_num = chapter_match.group(1) or chapter_match.group(2)\n",
    "                chapter_key = f\"{chapter_num} CHAPTER\"\n",
    "\n",
    "                # Avoid duplicates (TOC chapters)\n",
    "                if chapter_key in chapters_seen:\n",
    "                    continue\n",
    "                chapters_seen.add(chapter_key)\n",
    "\n",
    "                # Save previous chapter/heading content\n",
    "                if current_chapter:\n",
    "                    if current_heading and buffer.strip():\n",
    "                        structured_content[current_chapter][current_heading] = buffer.strip()\n",
    "                    elif buffer.strip():\n",
    "                        structured_content[current_chapter][\"content\"] = buffer.strip()\n",
    "\n",
    "                # Initialize new chapter\n",
    "                current_chapter = chapter_key\n",
    "                structured_content[current_chapter] = {}\n",
    "                current_heading = None\n",
    "                buffer = \"\"\n",
    "                print(f\"ðŸŸ¢ Detected Chapter: {current_chapter}\")\n",
    "                continue  # Next line\n",
    "\n",
    "            # ---------- Detect Section/Heading ----------\n",
    "            heading_match = heading_pattern.match(line_clean)\n",
    "            if heading_match:\n",
    "                # Save previous heading content\n",
    "                if current_heading and buffer.strip():\n",
    "                    structured_content[current_chapter][current_heading] = buffer.strip()\n",
    "                elif not current_heading and buffer.strip():\n",
    "                    structured_content[current_chapter][\"content\"] = buffer.strip()\n",
    "\n",
    "                # Clean and set heading\n",
    "                heading_number = heading_match.group(1)\n",
    "                heading_title = clean_heading(heading_match.group(2).strip())\n",
    "                current_heading = f\"{heading_number} {heading_title}\"\n",
    "                buffer = \"\"  # Reset for new section\n",
    "                print(f\"ðŸ”µ Detected Heading: {current_heading}\")\n",
    "                continue  # Next line\n",
    "\n",
    "            # ---------- Accumulate Content ----------\n",
    "            buffer += \" \" + line_clean  # Add content\n",
    "\n",
    "    # ---------- Final buffer save ----------\n",
    "    if current_chapter:\n",
    "        if current_heading and buffer.strip():\n",
    "            structured_content[current_chapter][current_heading] = buffer.strip()\n",
    "        elif buffer.strip():\n",
    "            structured_content[current_chapter][\"content\"] = buffer.strip()\n",
    "\n",
    "    return structured_content\n",
    "\n",
    "\n",
    "# ------------------ PRINT FUNCTION ------------------\n",
    "def print_structured_content(structured_content):\n",
    "    for chapter, sections in structured_content.items():\n",
    "        print(f\"\\n=== {chapter} ===\\n\")\n",
    "        for heading, content in sections.items():\n",
    "            print(f\"--- {heading} ---\\n\")\n",
    "            print(content[:500])  # Print first 500 chars for preview\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "# ------------------ SAVE TO JSON FUNCTION ------------------\n",
    "def save_to_json(data, file_path):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nâœ… Data successfully saved to {file_path}\")\n",
    "\n",
    "\n",
    "# ------------------ PDF TEXT EXTRACTION FUNCTION ------------------\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from PDF using PyPDF2.\n",
    "    \"\"\"\n",
    "    extracted_pages = []\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page_number, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            extracted_pages.append(page_text if page_text else \"\")\n",
    "    return extracted_pages\n",
    "\n",
    "\n",
    "# ------------------ MAIN EXECUTION ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"ex.pdf\"  # âœ… Path to your PDF file\n",
    "    output_json_path = \"structured_science_text.json\"  # âœ… Output path for JSON\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    pages_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "    # Step 2: Parse chapters and sections (with cleaning)\n",
    "    structured_content = parse_chapters_and_sections(pages_text)\n",
    "\n",
    "    # Step 3: Print preview in console\n",
    "    print_structured_content(structured_content)\n",
    "\n",
    "    # Step 4: Save structured output as JSON\n",
    "    save_to_json(structured_content, output_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafde00b-ee71-4829-b655-38c565a6429d",
   "metadata": {},
   "source": [
    "# processing chapters âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249eed48-b2a0-4b25-9e93-5ae637a63787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 10 PDFs: ['chap1.pdf', 'chap12.pdf', 'chap13.pdf', 'chap15.pdf', 'chap2.pdf', 'chap3.pdf', 'chap6.pdf', 'chap7.pdf', 'ex.pdf', '~$ex.pdf']\n",
      "\n",
      "ðŸš€ Extracting text from chap1.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap1_output.json\n",
      "âœ… Processed: chap1.pdf â†’ Saved as processed_textbook\\chap1_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap12.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap12_output.json\n",
      "âœ… Processed: chap12.pdf â†’ Saved as processed_textbook\\chap12_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap13.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap13_output.json\n",
      "âœ… Processed: chap13.pdf â†’ Saved as processed_textbook\\chap13_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap15.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap15_output.json\n",
      "âœ… Processed: chap15.pdf â†’ Saved as processed_textbook\\chap15_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap2.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap2_output.json\n",
      "âœ… Processed: chap2.pdf â†’ Saved as processed_textbook\\chap2_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap3.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap3_output.json\n",
      "âœ… Processed: chap3.pdf â†’ Saved as processed_textbook\\chap3_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap6.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap6_output.json\n",
      "âœ… Processed: chap6.pdf â†’ Saved as processed_textbook\\chap6_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap7.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap7_output.json\n",
      "âœ… Processed: chap7.pdf â†’ Saved as processed_textbook\\chap7_output.json\n",
      "\n",
      "ðŸš€ Extracting text from ex.pdf...\n",
      "âœ… Data saved to processed_textbook\\ex_output.json\n",
      "âœ… Processed: ex.pdf â†’ Saved as processed_textbook\\ex_output.json\n",
      "\n",
      "ðŸš€ Extracting text from ~$ex.pdf...\n"
     ]
    },
    {
     "ename": "FileDataError",
     "evalue": "Failed to open file 'textbooks\\\\~$ex.pdf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFzErrorFormat\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python312\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   3037\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3038\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3039\u001b[1;33m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python312\\Lib\\site-packages\\pymupdf\\mupdf.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m  46643\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mto\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  46644\u001b[0m     \"\"\"\n\u001b[1;32m> 46645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_mupdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfz_open_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFzErrorFormat\u001b[0m: code=7: no objects found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileDataError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_716\\2761977665.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0minput_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"textbooks\"\u001b[0m  \u001b[1;31m# Folder containing chap1.pdf, chap2.pdf, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0moutput_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"processed_textbook\"\u001b[0m  \u001b[1;31m# Folder to save output JSONs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mprocess_textbook_pdfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_716\\2761977665.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_folder, output_folder)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m\\nðŸš€ Extracting text from \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Step 1: Extract structured text from PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mstructured_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_with_markers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstructured_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mâš ï¸ Warning: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m appears to be empty!\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_716\\2761977665.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mUses\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlargest\u001b[0m \u001b[0mfont\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmain\u001b[0m \u001b[0mheadings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mUses\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msecond\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlargest\u001b[0m \u001b[0mfont\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msubheadings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mSkips\u001b[0m \u001b[0munwanted\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mlike\u001b[0m \u001b[1;34m\"Activity X.Y\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"Figure X.Y\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mextracted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mfont_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python312\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   3035\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_pdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3036\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3038\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3039\u001b[1;33m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileDataError\u001b[0m: Failed to open file 'textbooks\\\\~$ex.pdf'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# ------------------ EXTRACT TEXT WITH HEADINGS & SUBHEADINGS ------------------\n",
    "\n",
    "def extract_text_with_markers(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF and tags headings/subheadings dynamically based on font size.\n",
    "    - Uses the largest font size for main headings.\n",
    "    - Uses the second-largest font size for subheadings.\n",
    "    - Skips unwanted labels like \"Activity X.Y\" and \"Figure X.Y\".\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = []\n",
    "    font_sizes = []\n",
    "\n",
    "    # First Pass: Collect font sizes\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        font_sizes.append(span[\"size\"])\n",
    "\n",
    "    # Find unique font sizes and determine thresholds\n",
    "    unique_sizes = sorted(set(font_sizes), reverse=True)  # Sort from largest to smallest\n",
    "    main_heading_size = unique_sizes[0] if len(unique_sizes) > 0 else 15\n",
    "    subheading_size = unique_sizes[1] if len(unique_sizes) > 1 else 12  # Second-largest font\n",
    "\n",
    "    # Second Pass: Extract text with markers\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    text = \" \".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "                    if not text:\n",
    "                        continue  # Skip empty lines\n",
    "                    \n",
    "                    # Extract font size and style\n",
    "                    font_size = line[\"spans\"][0][\"size\"]\n",
    "                    font_name = line[\"spans\"][0][\"font\"]\n",
    "\n",
    "                    # âœ… Ignore \"Activity X.Y\" or \"Figure X.Y\" as headings\n",
    "                    if re.match(r\"^(Activity|Figure)\\s*\\d+(\\.\\d+)?\", text, re.IGNORECASE):\n",
    "                        continue  \n",
    "\n",
    "                    # âœ… Detect **Main Headings** (Largest Font Size)\n",
    "                    if font_size >= main_heading_size:\n",
    "                        extracted_text.append(f\"\\n#HEADING# {text}\\n\")\n",
    "                    \n",
    "                    # âœ… Detect **Subheadings** (Second-Largest Font Size, Bold)\n",
    "                    elif font_size >= subheading_size and \"Bold\" in font_name:\n",
    "                        extracted_text.append(f\"\\n@SUBHEADING@ {text}\\n\")\n",
    "\n",
    "                    # âœ… Normal text (Content)\n",
    "                    else:\n",
    "                        extracted_text.append(text)\n",
    "\n",
    "    return \"\\n\".join(extracted_text)  # Convert list to single text block\n",
    "\n",
    "\n",
    "# ------------------ SAVE TO JSON FUNCTION ------------------\n",
    "\n",
    "def save_to_json(data, file_path):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… Data saved to {file_path}\")\n",
    "\n",
    "\n",
    "# ------------------ PROCESS MULTIPLE PDF FILES ------------------\n",
    "\n",
    "def process_textbook_pdfs(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Loops through all PDFs, extracts text, cleans it, and saves output as JSON.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "    files_found = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"âŒ No PDF files found in the 'textbooks/' folder!\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ“‚ Found {len(files_found)} PDFs: {files_found}\")\n",
    "\n",
    "    for filename in files_found:\n",
    "        pdf_path = os.path.join(input_folder, filename)\n",
    "        chapter_number = filename.replace(\".pdf\", \"\")  # Extract 'chap1', 'chap2', etc.\n",
    "        chapter_name = f\"Chapter {chapter_number}\"  # Format as 'Chapter 1'\n",
    "\n",
    "        print(f\"\\nðŸš€ Extracting text from {filename}...\")\n",
    "\n",
    "        # Step 1: Extract structured text from PDF\n",
    "        structured_text = extract_text_with_markers(pdf_path)\n",
    "\n",
    "        if not structured_text.strip():\n",
    "            print(f\"âš ï¸ Warning: {filename} appears to be empty!\")\n",
    "\n",
    "        # Step 2: Save as JSON\n",
    "        output_file = os.path.join(output_folder, f\"{chapter_number}_output.json\")\n",
    "        save_to_json({chapter_name: structured_text}, output_file)\n",
    "\n",
    "        print(f\"âœ… Processed: {filename} â†’ Saved as {output_file}\")\n",
    "\n",
    "\n",
    "# ------------------ MAIN EXECUTION ------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"textbooks\"  # Folder containing chap1.pdf, chap2.pdf, etc.\n",
    "    output_folder = \"processed_textbook\"  # Folder to save output JSONs\n",
    "\n",
    "    process_textbook_pdfs(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e835a9-33ed-499a-9c3a-ae2f38df0b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3475b-9656-4d4c-a20f-23863363c611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639c17d-c37c-49e3-8060-ab0beebabe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdbaa5-7650-4ec6-bc82-67c6e890a5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e07344c2-b6a5-47bc-b4ff-5b5db587cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 8 PDFs: ['chap1.pdf', 'chap12.pdf', 'chap13.pdf', 'chap15.pdf', 'chap2.pdf', 'chap3.pdf', 'chap6.pdf', 'chap7.pdf']\n",
      "\n",
      "ðŸš€ Extracting text from chap1.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap1_output.json\n",
      "âœ… Processed: chap1.pdf â†’ Saved as processed_textbook\\chap1_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap12.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap12_output.json\n",
      "âœ… Processed: chap12.pdf â†’ Saved as processed_textbook\\chap12_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap13.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap13_output.json\n",
      "âœ… Processed: chap13.pdf â†’ Saved as processed_textbook\\chap13_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap15.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap15_output.json\n",
      "âœ… Processed: chap15.pdf â†’ Saved as processed_textbook\\chap15_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap2.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap2_output.json\n",
      "âœ… Processed: chap2.pdf â†’ Saved as processed_textbook\\chap2_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap3.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap3_output.json\n",
      "âœ… Processed: chap3.pdf â†’ Saved as processed_textbook\\chap3_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap6.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap6_output.json\n",
      "âœ… Processed: chap6.pdf â†’ Saved as processed_textbook\\chap6_output.json\n",
      "\n",
      "ðŸš€ Extracting text from chap7.pdf...\n",
      "âœ… Data saved to processed_textbook\\chap7_output.json\n",
      "âœ… Processed: chap7.pdf â†’ Saved as processed_textbook\\chap7_output.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# ------------------ CLEAN TEXT FUNCTION ------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans extracted text by removing duplicate words, phrases, and fixing common OCR errors.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\b(\\w+\\s*\\d*\\.?\\d*)\\s*(\\1)+\\b', r'\\1', text, flags=re.IGNORECASE)  # Remove repeated words\n",
    "    text = re.sub(r'(\\b[\\w\\s,\\'\"-]+[.!?])\\s*(\\1)+', r'\\1', text, flags=re.IGNORECASE)  # Remove repeated sentences\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Replace multiple newlines with one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# ------------------ EXTRACT TEXT FROM PDF (WITH MARKERS) ------------------\n",
    "def extract_text_with_markers(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from PDF and marks headings and subheadings with special symbols.\n",
    "    - Uses font size to detect headings/subheadings.\n",
    "    - Uses bold styling to differentiate subheadings.\n",
    "    - Preserves content structure for better chunking later.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = []\n",
    "    font_sizes = []\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    text = \" \".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "\n",
    "                    if not text:\n",
    "                        continue  # Skip empty lines\n",
    "\n",
    "                    font_size = line[\"spans\"][0][\"size\"]\n",
    "                    font_name = line[\"spans\"][0][\"font\"]\n",
    "                    font_sizes.append(font_size)\n",
    "\n",
    "                    # âœ… Detecting **Main Headings** (Pink, Largest Font)\n",
    "                    if font_size > max(font_sizes) * 0.9:  \n",
    "                        extracted_text.append(f\"\\n#HEADING# {text}\\n\")\n",
    "\n",
    "                    # âœ… Detecting **Subheadings** (Light Gray/Bold)\n",
    "                    elif font_size > max(font_sizes) * 0.7 and \"Bold\" in font_name:\n",
    "                        extracted_text.append(f\"\\n@SUBHEADING@ {text}\\n\")\n",
    "\n",
    "                    # âœ… Normal text (Content)\n",
    "                    else:\n",
    "                        extracted_text.append(text)\n",
    "\n",
    "    return \"\\n\".join(extracted_text)\n",
    "\n",
    "# ------------------ SAVE TO JSON FUNCTION ------------------\n",
    "def save_to_json(data, file_path):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… Data saved to {file_path}\")\n",
    "\n",
    "# ------------------ PROCESS MULTIPLE PDF FILES ------------------\n",
    "def process_textbook_pdfs(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Loops through all PDFs, extracts text, cleans it, and saves output as JSON.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "    files_found = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\") and not f.startswith(\"~$\")]\n",
    "\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"âŒ No PDF files found in the 'textbooks/' folder!\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ“‚ Found {len(files_found)} PDFs: {files_found}\")\n",
    "\n",
    "    for filename in files_found:\n",
    "        pdf_path = os.path.join(input_folder, filename)\n",
    "        chapter_number = filename.split(\".pdf\")[0]  # Extract 'chap1', 'chap2', etc.\n",
    "        chapter_name = f\"Chapter {chapter_number}\"  # Format as 'Chapter 1'\n",
    "\n",
    "        print(f\"\\nðŸš€ Extracting text from {filename}...\")\n",
    "\n",
    "        # Step 1: Extract structured text from PDF\n",
    "        structured_text = extract_text_with_markers(pdf_path)\n",
    "\n",
    "        if not structured_text.strip():\n",
    "            print(f\"âš ï¸ Warning: {filename} appears to be empty!\")\n",
    "\n",
    "        # Step 2: Structure the content\n",
    "        structured_data = {chapter_name: structured_text}\n",
    "\n",
    "        # Step 3: Save as JSON\n",
    "        output_file = os.path.join(output_folder, f\"{chapter_number}_output.json\")\n",
    "        save_to_json(structured_data, output_file)\n",
    "\n",
    "        print(f\"âœ… Processed: {filename} â†’ Saved as {output_file}\")\n",
    "\n",
    "# ------------------ MAIN EXECUTION ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"textbooks\"  # Folder containing chap1.pdf, chap2.pdf, etc.\n",
    "    output_folder = \"processed_textbook\"  # Folder to save output JSONs\n",
    "\n",
    "    process_textbook_pdfs(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02278f46-e720-49db-8b28-ebc176ad0875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaeaea-db50-4914-8469-36f6781d2807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6aa1d2-6ee7-44e0-828b-dff1a250f7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dff3c-eddd-4be8-a3bb-caefeccc29fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4641a-2d36-4de2-985a-7fc5738e4825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9472488b-9a80-41c0-931e-f3244b8dd6cc",
   "metadata": {},
   "source": [
    "# Refining âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499c0c9b-8c0a-4456-ac42-46cfd139d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Processing chap12_output.json...\n",
      "âœ… Processed chap12_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap13_output.json...\n",
      "âœ… Processed chap13_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap15_output.json...\n",
      "âœ… Processed chap15_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap1_output.json...\n",
      "âœ… Processed chap1_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap2_output.json...\n",
      "âœ… Processed chap2_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap3_output.json...\n",
      "âœ… Processed chap3_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap6_output.json...\n",
      "âœ… Processed chap6_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing chap7_output.json...\n",
      "âœ… Processed chap7_output.json â†’ Saved cleaned text & structured JSON.\n",
      "\n",
      "ðŸš€ Processing ex_output.json...\n",
      "âœ… Processed ex_output.json â†’ Saved cleaned text & structured JSON.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ---------------------- CLEANING FUNCTIONS -----------------------\n",
    "\n",
    "def remove_repeated_phrases(text):\n",
    "    \"\"\"Remove repeated phrases (e.g., Activity 1.1 repeated)\"\"\"\n",
    "    return re.sub(r'\\b(\\w+(?:\\s+\\w+){0,4})\\b(?:\\s+\\1\\b)+', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def fix_spacing_and_formatting(text):\n",
    "    \"\"\"Fix bullet points, spacing, and line breaks.\"\"\"\n",
    "    text = re.sub(r'\\bn\\s*', '\\n- ', text)  # Replace 'n' with bullet points\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_broken_words(text):\n",
    "    \"\"\"Fix broken words split by OCR.\"\"\"\n",
    "    text = re.sub(r'\\bn\\s+', ' ', text)  # Remove standalone 'n'\n",
    "    text = re.sub(r'(\\w)-\\s*(\\w)', r'\\1\\2', text)  # Fix hyphenated broken words\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_headings(text):\n",
    "    \"\"\"Clean broken headings and duplicates.\"\"\"\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d+)+)\\s+([A-Z]+(?:\\s+[A-Z]+)+)', \n",
    "                  lambda m: m.group(1) + \" \" + m.group(2).replace(' ', ''), text)\n",
    "    text = re.sub(r'\\b(\\d+(?:\\.\\d+)+.*?)\\1\\b', r'\\1', text)  # Remove duplicated headings\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_figure_activity_repeats(text):\n",
    "    \"\"\"Remove repeated figure and activity captions.\"\"\"\n",
    "    text = re.sub(r'(Figure\\s+\\d+\\.\\d+)(\\s+\\1)+', r'\\1', text)\n",
    "    text = re.sub(r'(Activity\\s+\\d+\\.\\d+)(\\s+\\1)+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def format_equations(text):\n",
    "    \"\"\"Standardize equations.\"\"\"\n",
    "    text = re.sub(r'([A-Za-z0-9\\(\\)]+)\\s*[-â€“>]\\s*([A-Za-z0-9\\(\\)]+)', r'\\1 â†’ \\2', text)\n",
    "    text = re.sub(r'((?:[A-Za-z0-9\\(\\)]+\\s*\\+\\s*)+[A-Za-z0-9\\(\\)]+)\\s*â†’\\s*((?:[A-Za-z0-9\\(\\)]+\\s*\\+\\s*)*[A-Za-z0-9\\(\\)]+)', r'\\n\\1 â†’ \\2\\n', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_section_breaks(text):\n",
    "    \"\"\"Add line breaks before headings for better segmentation.\"\"\"\n",
    "    return re.sub(r'(\\n*)(\\d+(?:\\.\\d+)*\\s+[A-Z][^\\n]+)', r'\\n\\n\\2\\n', text)\n",
    "\n",
    "\n",
    "def remove_ktbs_notices(text):\n",
    "    \"\"\"Remove KTBS copyright notices.\"\"\"\n",
    "    return re.sub(r'Â©KTBS Not to be re published(?: Science)?', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def final_cleanup(text):\n",
    "    \"\"\"Final text cleaning for common broken words & unwanted characters.\"\"\"\n",
    "    text = re.sub(r'\\bâ†’\\s*t', 't', text)  # Fix 'â†’ t' as 't'\n",
    "    text = re.sub(r'\\bâ†’\\s*n', 'n', text)  # Fix 'â†’ n' as 'n'\n",
    "    text = re.sub(r'\\bâ†’\\s*ature', 'nature', text)  # Fix 'â†’ ature' as 'nature'\n",
    "    text = re.sub(r'\\bâ†’\\s*eeds', 'needs', text)  # Fix 'â†’ eeds' as 'needs'\n",
    "    text = re.sub(r'\\bâ†’\\s*itrate', 'nitrate', text)  # Fix 'â†’ itrate' as 'nitrate'\n",
    "    text = re.sub(r'\\bail\\b', 'nail', text)  # Fix 'ail' to 'nail'\n",
    "    text = re.sub(r'\\blear\\s*â†’\\s*t\\b', 'learnt', text)  # Fix 'lear â†’ t' as 'learnt'\n",
    "    \n",
    "    # Fix broken \"not\"\n",
    "    text = re.sub(r'â†’\\s*ot', 'not', text)\n",
    "    \n",
    "    # Remove repeated figures/activities\n",
    "    text = re.sub(r'(Figure\\s+\\d+\\.\\d+)(\\s*\\1)+', r'\\1', text)\n",
    "    text = re.sub(r'(Activity\\s+\\d+\\.\\d+)(\\s*\\1)+', r'\\1', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_full_text(text):\n",
    "    \"\"\"Apply all text cleaning steps.\"\"\"\n",
    "    text = remove_ktbs_notices(text)  \n",
    "    text = remove_repeated_phrases(text)\n",
    "    text = fix_spacing_and_formatting(text)\n",
    "    text = clean_broken_words(text)\n",
    "    text = clean_headings(text)\n",
    "    text = remove_figure_activity_repeats(text)\n",
    "    text = format_equations(text)\n",
    "    text = add_section_breaks(text)\n",
    "    text = final_cleanup(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------------- STRUCTURE TEXT TO JSON -----------------------\n",
    "\n",
    "def structure_text_to_json(text):\n",
    "    \"\"\"\n",
    "    Splits cleaned text into headings and their corresponding content, preserving order.\n",
    "    \"\"\"\n",
    "    structured_data = {}\n",
    "    current_heading = \"INTRODUCTION\"  # Default starting heading\n",
    "    buffer = \"\"\n",
    "\n",
    "    heading_pattern = re.compile(r'^(\\d+(?:\\.\\d+)+\\s+.*)', re.MULTILINE)\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        heading_match = heading_pattern.match(line)\n",
    "        if heading_match:\n",
    "            if buffer:\n",
    "                structured_data[current_heading] = buffer.strip()\n",
    "                buffer = \"\"\n",
    "            current_heading = heading_match.group(1).strip()\n",
    "        else:\n",
    "            buffer += \" \" + line  \n",
    "\n",
    "    if buffer:\n",
    "        structured_data[current_heading] = buffer.strip()\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # âœ… Input and Output Paths\n",
    "    input_folder = \"processed_textbook\"  # Folder with cleaned JSON files\n",
    "    cleaned_text_folder = \"cleaned_output\"  # Folder to save cleaned text\n",
    "    structured_output_folder = \"structured_output\"  # Folder to save structured JSONs\n",
    "\n",
    "    os.makedirs(cleaned_text_folder, exist_ok=True)\n",
    "    os.makedirs(structured_output_folder, exist_ok=True)\n",
    "\n",
    "    # âœ… Loop through each JSON file in input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            chapter_name = filename.replace(\"_output.json\", \"\")  # Extract \"chap1\" from \"chap1_output.json\"\n",
    "\n",
    "            print(f\"\\nðŸš€ Processing {filename}...\")\n",
    "\n",
    "            # Step 1: Load JSON File\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                json_data = json.load(file)\n",
    "\n",
    "            # Extract text (assumes JSON format {\"Chapter X\": \"Text content\"})\n",
    "            chapter_text = list(json_data.values())[0]  \n",
    "\n",
    "            # Step 2: Clean text\n",
    "            cleaned_text = clean_full_text(chapter_text)\n",
    "\n",
    "            # Step 3: Save cleaned text\n",
    "            cleaned_text_path = os.path.join(cleaned_text_folder, f\"{chapter_name}_cleaned.txt\")\n",
    "            with open(cleaned_text_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(cleaned_text)\n",
    "\n",
    "            # Step 4: Structure text into JSON\n",
    "            structured_json = structure_text_to_json(cleaned_text)\n",
    "\n",
    "            # Step 5: Save structured JSON\n",
    "            structured_json_path = os.path.join(structured_output_folder, f\"{chapter_name}_structured.json\")\n",
    "            with open(structured_json_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(structured_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(f\"âœ… Processed {filename} â†’ Saved cleaned text & structured JSON.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3775e6-c4c2-4f40-a469-196306be6929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ddfe7-2b05-4c88-b9a2-9fb90d5d9ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d173d-5f70-4e23-bb1e-526afb506a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0810ed-3d40-42bd-8a2a-cd2002ea3727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ad4f61-8db0-4730-906a-8b6c5d27d5c1",
   "metadata": {},
   "source": [
    "# Chunking of chapter files âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cdc92b6-0a8f-4010-bc84-0cdc19642f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunked file saved: chunked_output\\chap12_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap13_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap15_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap1_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap2_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap3_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap6_chunked.json\n",
      "âœ… Chunked file saved: chunked_output\\chap7_chunked.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ---------------------- CHUNKING FUNCTION -----------------------\n",
    "\n",
    "def chunk_text(text):\n",
    "    \"\"\"\n",
    "    Splits text into structured chunks based on subheadings.\n",
    "    \"\"\"\n",
    "    structured_data = {}\n",
    "    current_section = \"Introduction\"\n",
    "    buffer = \"\"\n",
    "\n",
    "    # Pattern to detect valid subheadings (e.g., \"1.1 Chemical Reactions\")\n",
    "    heading_pattern = re.compile(r'(\\d+\\.\\d+(?:\\.\\d+)*\\s+[A-Z][^\\n]*)')\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "\n",
    "        # Ignore empty lines\n",
    "        if not line_clean:\n",
    "            continue\n",
    "\n",
    "        # Detect valid subheadings\n",
    "        heading_match = heading_pattern.match(line_clean)\n",
    "        if heading_match:\n",
    "            new_section = heading_match.group(1).strip()\n",
    "\n",
    "            # Ignore headings with \"Activity\" or \"Figure\"\n",
    "            if \"Activity\" in new_section or \"Figure\" in new_section:\n",
    "                continue\n",
    "\n",
    "            # Store previous section content\n",
    "            if buffer.strip():\n",
    "                structured_data[current_section] = buffer.strip()\n",
    "\n",
    "            # Start new section\n",
    "            current_section = new_section\n",
    "            buffer = \"\"\n",
    "        else:\n",
    "            buffer += \" \" + line_clean  # Accumulate content\n",
    "\n",
    "    # Store last section\n",
    "    if buffer.strip():\n",
    "        structured_data[current_section] = buffer.strip()\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "\n",
    "def process_cleaned_file(input_path, output_path, chapter_name):\n",
    "    \"\"\"\n",
    "    Reads a cleaned text file, chunks it by subheadings, and saves it as JSON.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Process the text into structured chunks\n",
    "    structured_data = chunk_text(text)\n",
    "\n",
    "    # Save the structured chunked file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({chapter_name: structured_data}, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… Chunked file saved: {output_path}\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"cleaned_output\"  # Folder with cleaned text files\n",
    "    output_folder = \"chunked_output\"  # Folder to save chunked JSONs\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\"_cleaned.txt\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            chapter_name = filename.replace(\"_cleaned.txt\", \"\")  # Extract \"chap1\"\n",
    "\n",
    "            output_path = os.path.join(output_folder, f\"{chapter_name}_chunked.json\")\n",
    "            process_cleaned_file(input_path, output_path, chapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316052d-ea02-4887-9fa4-a7cc8682e024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d2c21-deaf-403b-b963-e469e2371853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5a1e8d7-c25a-4636-8875-0e4593ef46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap12_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap13_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap15_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap1_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap2_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap3_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap6_chunked_fixed.json\n",
      "âœ… Fixed chunked file saved: fixed_chunked_output\\chap7_chunked_fixed.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ---------------------- FUNCTION TO CLEAN SUBHEADINGS -----------------------\n",
    "\n",
    "def clean_subheading(subheading):\n",
    "    \"\"\"\n",
    "    Extracts only the heading number + short title (removes long sentences, duplicate words, and unnecessary text).\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+\\.\\d+)\\s+([A-Za-z\\s-]+)', subheading)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} {match.group(2).strip().split()[0]}\"  # Keep only main topic\n",
    "    return subheading.strip()\n",
    "\n",
    "# ---------------------- FUNCTION TO FIX CHUNKED JSON -----------------------\n",
    "\n",
    "def fix_chunked_json(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads a chunked JSON file, cleans subheading names, and saves a structured output.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunked_data = json.load(file)\n",
    "\n",
    "    fixed_data = {}\n",
    "    for chapter, sections in chunked_data.items():\n",
    "        fixed_data[chapter] = {}\n",
    "\n",
    "        for subheading, chunks in sections.items():\n",
    "            cleaned_subheading = clean_subheading(subheading)\n",
    "            fixed_data[chapter][cleaned_subheading] = chunks\n",
    "\n",
    "    # Save the cleaned JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(fixed_data, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… Fixed chunked file saved: {output_path}\")\n",
    "\n",
    "# ---------------------- MAIN EXECUTION -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"chunked_output\"  # Folder with chunked JSON files\n",
    "    output_folder = \"fixed_chunked_output\"  # Folder to save fixed JSONs\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\"_chunked.json\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename.replace(\"_chunked\", \"_chunked_fixed\"))\n",
    "\n",
    "            fix_chunked_json(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce83ad-0f57-4716-b081-4b0c912f5eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38293d88-5de3-429c-ac24-42cc0e1bd0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4c9e56-b6c5-4f1d-bc46-202eb27a4704",
   "metadata": {},
   "source": [
    "# Combining Chunked chapters âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f369137a-cdde-4204-81b4-d6483e510924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged JSON saved: all_chapters_chunked.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ---------------------- FUNCTION TO MERGE ALL CHAPTER JSONs -----------------------\n",
    "\n",
    "def merge_all_chapters(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Merges all chapter-wise chunked JSON files into a single JSON file.\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "\n",
    "    for filename in sorted(os.listdir(input_folder)):  # Ensure correct chapter order\n",
    "        if filename.endswith(\"_chunked_fixed.json\"):\n",
    "            chapter_name = filename.replace(\"_chunked_fixed.json\", \"\")  # Extract \"chap1\", \"chap2\", etc.\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                chapter_data = json.load(file)\n",
    "\n",
    "            merged_data[chapter_name] = chapter_data.get(chapter_name, {})  # Keep only chapter content\n",
    "\n",
    "    # Save merged JSON\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(merged_data, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… Merged JSON saved: {output_file}\")\n",
    "\n",
    "# ---------------------- MAIN EXECUTION -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"fixed_chunked_output\"  # Folder with fixed chunked JSON files\n",
    "    output_file = \"all_chapters_chunked.json\"  # Final merged JSON file\n",
    "\n",
    "    merge_all_chapters(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00031b9-96f5-43b6-accf-5527e0af8757",
   "metadata": {},
   "source": [
    "# Final refinement âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb1f9313-0d38-4305-a901-4ae95add015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed RAG-ready JSON saved: knowledgebase.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ---------------------- FUNCTION TO CLEAN SUBHEADINGS -----------------------\n",
    "\n",
    "def clean_subheading(subheading):\n",
    "    \"\"\"\n",
    "    Extracts only the heading number + short title (removes long sentences, duplicate words, and unnecessary text).\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+\\.\\d+)\\s+([A-Za-z\\s-]+)', subheading)\n",
    "    if match:\n",
    "        return f\"{match.group(1)} {match.group(2).strip().split()[0]}\"  # Keep only the main topic word\n",
    "    return subheading.strip()\n",
    "\n",
    "\n",
    "# ---------------------- FUNCTION TO CLEAN CONTENT -----------------------\n",
    "\n",
    "def clean_content(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing extra spaces, fixing broken words, and removing redundant symbols.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'(\\w)-\\s*(\\w)', r'\\1\\2', text)  # Fix hyphenated words split by OCR\n",
    "    text = text.replace(\"Â©KTBS\", \"\").replace(\"Not to be republished\", \"\").strip()  # Remove unwanted text\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------------- FUNCTION TO FIX CHUNKED JSON -----------------------\n",
    "\n",
    "def fix_chunked_json(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads a chunked JSON file, cleans subheading names, formats content, and saves a structured output.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunked_data = json.load(file)\n",
    "\n",
    "    fixed_data = {}\n",
    "    for chapter, sections in chunked_data.items():\n",
    "        fixed_data[chapter] = {}\n",
    "\n",
    "        for subheading, chunks in sections.items():\n",
    "            cleaned_subheading = clean_subheading(subheading)\n",
    "            cleaned_chunks = [clean_content(chunk) for chunk in chunks]\n",
    "\n",
    "            fixed_data[chapter][cleaned_subheading] = cleaned_chunks\n",
    "\n",
    "    # Save the cleaned JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(fixed_data, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"âœ… Fixed RAG-ready JSON saved: {output_path}\")\n",
    "\n",
    "\n",
    "# ---------------------- MAIN EXECUTION -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"all_chapters_chunked.json\"  # Input merged JSON file\n",
    "    output_file = \"knowledgebase.json\"  # Final cleaned JSON for RAG\n",
    "\n",
    "    fix_chunked_json(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e209c58-f6fa-4af8-8caa-f80215c2f6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
